{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.chdir(\"/workspaces/OmicsCopilot/src/server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "from pydantic_settings import BaseSettings\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Callable\n",
    "import os\n",
    "\n",
    "import data.load\n",
    "\n",
    "\n",
    "from pydantic_settings import BaseSettings\n",
    "from pydantic import Field\n",
    "\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class Settings(BaseSettings):\n",
    "    MODEL_PATH: Path\n",
    "\n",
    "    EMBEDDING_MODEL: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    ONNX_EMBEDDING_MODEL: Optional[Path] = Field(None, env=\"ONNX_EMBEDDING_MODEL\")\n",
    "\n",
    "\n",
    "settings = Settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.ONNX_EMBEDDING_MODEL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-small-en-v1.5'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.EMBEDDING_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import AutoOptimizationConfig, ORTConfig, ORTModelForFeatureExtraction, ORTOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimum-cli export onnx \\\n",
    "#         --model \"models/${model}\" \\\n",
    "#         --framework pt \\\n",
    "#         --task feature-extraction \\\n",
    "#         --optimize O4 \\\n",
    "#         --device cuda \\\n",
    "#         \"models/${model}-onnx-O4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-small-en-v1.5'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.EMBEDDING_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BAAI/bge-small-en-v1.5'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings.EMBEDDING_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n",
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.1+cu118\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP Error /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 100: no CUDA-capable device is detected ; GPU=1092523520 ; hostname=160e4b733ea3 ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=236 ; expr=cudaSetDevice(info_.device_id); \n",
      "\n",
      " when using ['CUDAExecutionProvider']\n",
      "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 100: no CUDA-capable device is detected ; GPU=-1368988165 ; hostname=160e4b733ea3 ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=236 ; expr=cudaSetDevice(info_.device_id); \n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:419\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    420\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mRuntimeError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:463\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m sess\u001b[39m.\u001b[39;49minitialize_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    465\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess \u001b[39m=\u001b[39m sess\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 100: no CUDA-capable device is detected ; GPU=1092523520 ; hostname=160e4b733ea3 ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=236 ; expr=cudaSetDevice(info_.device_id); \n\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6f6d696373636f70696c6f742d7365727665722d31227d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m n \u001b[39m=\u001b[39m ORTModelForFeatureExtraction\u001b[39m.\u001b[39;49mfrom_pretrained(settings\u001b[39m.\u001b[39;49mEMBEDDING_MODEL, export\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, provider\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCUDAExecutionProvider\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/onnxruntime/modeling_ort.py:622\u001b[0m, in \u001b[0;36mORTModel.from_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, cache_dir, subfolder, config, local_files_only, provider, session_options, provider_options, use_io_binding, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    574\u001b[0m \u001b[39m@add_start_docstrings\u001b[39m(FROM_PRETRAINED_START_DOCSTRING)\n\u001b[1;32m    575\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    590\u001b[0m ):\n\u001b[1;32m    591\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39m    provider (`str`, defaults to `\"CPUExecutionProvider\"`):\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[39m        ONNX Runtime provider to use for loading the model. See https://onnxruntime.ai/docs/execution-providers/ for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[39m        `ORTModel`: The loaded ORTModel model.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    623\u001b[0m         model_id,\n\u001b[1;32m    624\u001b[0m         export\u001b[39m=\u001b[39;49mexport,\n\u001b[1;32m    625\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    626\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    627\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    628\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    629\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    630\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    631\u001b[0m         provider\u001b[39m=\u001b[39;49mprovider,\n\u001b[1;32m    632\u001b[0m         session_options\u001b[39m=\u001b[39;49msession_options,\n\u001b[1;32m    633\u001b[0m         provider_options\u001b[39m=\u001b[39;49mprovider_options,\n\u001b[1;32m    634\u001b[0m         use_io_binding\u001b[39m=\u001b[39;49muse_io_binding,\n\u001b[1;32m    635\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    636\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/modeling_base.py:372\u001b[0m, in \u001b[0;36mOptimizedModel.from_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m     trust_remote_code \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    371\u001b[0m from_pretrained_method \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_transformers \u001b[39mif\u001b[39;00m export \u001b[39melse\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained\n\u001b[0;32m--> 372\u001b[0m \u001b[39mreturn\u001b[39;00m from_pretrained_method(\n\u001b[1;32m    373\u001b[0m     model_id\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m    374\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    375\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    376\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    377\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    378\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    379\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    380\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    381\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    382\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/onnxruntime/modeling_ort.py:563\u001b[0m, in \u001b[0;36mORTModel._from_transformers\u001b[0;34m(cls, model_id, config, use_auth_token, revision, force_download, cache_dir, subfolder, local_files_only, trust_remote_code, provider, session_options, provider_options, use_io_binding, task)\u001b[0m\n\u001b[1;32m    560\u001b[0m config\u001b[39m.\u001b[39msave_pretrained(save_dir_path)\n\u001b[1;32m    561\u001b[0m maybe_save_preprocessors(model_id, save_dir_path, src_subfolder\u001b[39m=\u001b[39msubfolder)\n\u001b[0;32m--> 563\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m    564\u001b[0m     save_dir_path,\n\u001b[1;32m    565\u001b[0m     config,\n\u001b[1;32m    566\u001b[0m     use_io_binding\u001b[39m=\u001b[39;49muse_io_binding,\n\u001b[1;32m    567\u001b[0m     model_save_dir\u001b[39m=\u001b[39;49msave_dir,\n\u001b[1;32m    568\u001b[0m     provider\u001b[39m=\u001b[39;49mprovider,\n\u001b[1;32m    569\u001b[0m     session_options\u001b[39m=\u001b[39;49msession_options,\n\u001b[1;32m    570\u001b[0m     provider_options\u001b[39m=\u001b[39;49mprovider_options,\n\u001b[1;32m    571\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/onnxruntime/modeling_ort.py:506\u001b[0m, in \u001b[0;36mORTModel._from_pretrained\u001b[0;34m(cls, model_id, config, use_auth_token, revision, force_download, cache_dir, file_name, subfolder, local_files_only, provider, session_options, provider_options, use_io_binding, model_save_dir, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m model_save_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     model_save_dir \u001b[39m=\u001b[39m new_model_save_dir\n\u001b[0;32m--> 506\u001b[0m model \u001b[39m=\u001b[39m ORTModel\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m    507\u001b[0m     model_cache_path,\n\u001b[1;32m    508\u001b[0m     provider\u001b[39m=\u001b[39;49mprovider,\n\u001b[1;32m    509\u001b[0m     session_options\u001b[39m=\u001b[39;49msession_options,\n\u001b[1;32m    510\u001b[0m     provider_options\u001b[39m=\u001b[39;49mprovider_options,\n\u001b[1;32m    511\u001b[0m )\n\u001b[1;32m    513\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m(\n\u001b[1;32m    514\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m    515\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m     preprocessors\u001b[39m=\u001b[39mpreprocessors,\n\u001b[1;32m    519\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/onnxruntime/modeling_ort.py:369\u001b[0m, in \u001b[0;36mORTModel.load_model\u001b[0;34m(path, provider, session_options, provider_options)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     providers_options \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m \u001b[39mreturn\u001b[39;00m ort\u001b[39m.\u001b[39;49mInferenceSession(\n\u001b[1;32m    370\u001b[0m     path,\n\u001b[1;32m    371\u001b[0m     providers\u001b[39m=\u001b[39;49mproviders,\n\u001b[1;32m    372\u001b[0m     sess_options\u001b[39m=\u001b[39;49msession_options,\n\u001b[1;32m    373\u001b[0m     provider_options\u001b[39m=\u001b[39;49mproviders_options,\n\u001b[1;32m    374\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:430\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    429\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m fallback_error:\n\u001b[0;32m--> 430\u001b[0m         \u001b[39mraise\u001b[39;00m fallback_error \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39m# Fallback is disabled. Raise the original error.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:425\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEP Error \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m when using \u001b[39m\u001b[39m{\u001b[39;00mproviders\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    424\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFalling back to \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fallback_providers\u001b[39m}\u001b[39;00m\u001b[39m and retrying.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 425\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_inference_session(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fallback_providers, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    426\u001b[0m \u001b[39m# Fallback only once.\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable_fallback()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:463\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    460\u001b[0m     disabled_optimizers \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    462\u001b[0m \u001b[39m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m sess\u001b[39m.\u001b[39;49minitialize_session(providers, provider_options, disabled_optimizers)\n\u001b[1;32m    465\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess \u001b[39m=\u001b[39m sess\n\u001b[1;32m    466\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess_options \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sess\u001b[39m.\u001b[39msession_options\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:121 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] /onnxruntime_src/onnxruntime/core/providers/cuda/cuda_call.cc:114 std::conditional_t<THRW, void, onnxruntime::common::Status> onnxruntime::CudaCall(ERRTYPE, const char*, const char*, ERRTYPE, const char*, const char*, int) [with ERRTYPE = cudaError; bool THRW = true; std::conditional_t<THRW, void, onnxruntime::common::Status> = void] CUDA failure 100: no CUDA-capable device is detected ; GPU=-1368988165 ; hostname=160e4b733ea3 ; file=/onnxruntime_src/onnxruntime/core/providers/cuda/cuda_execution_provider.cc ; line=236 ; expr=cudaSetDevice(info_.device_id); \n\n"
     ]
    }
   ],
   "source": [
    "n = ORTModelForFeatureExtraction.from_pretrained(\n",
    "    settings.EMBEDDING_MODEL, export=True, provider=\"CUDAExecutionProvider\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab06d58853a4ad1bae372fc83d10f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = AutoModel.from_pretrained(settings.EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889f5b0fbef64997b1c5f9c5c4419147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a184ea7f894fddafa3b10c34e8f69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f8114dc4f454aeaa60c989ade3e63f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4458df4754eb4fe6b3404119adaf3be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a4cfda99484090b2d2388f1f7498ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find any ONNX model file in BAAI/bge-small-en-v1.5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6f6d696373636f70696c6f742d7365727665722d31227d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooling \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6f6d696373636f70696c6f742d7365727665722d31227d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mean_pooling\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6f6d696373636f70696c6f742d7365727665722d31227d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m embedding_model \u001b[39m=\u001b[39m EmbeddingModel()\n",
      "\u001b[1;32m/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6f6d696373636f70696c6f742d7365727665722d31227d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6f6d696373636f70696c6f742d7365727665722d31227d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(settings\u001b[39m.\u001b[39mEMBEDDING_MODEL)\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6f6d696373636f70696c6f742d7365727665722d31227d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m ORTModelForFeatureExtraction\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6f6d696373636f70696c6f742d7365727665722d31227d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m     settings\u001b[39m.\u001b[39;49mEMBEDDING_MODEL, provider\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCUDAExecutionProvider\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f6f6d696373636f70696c6f742d7365727665722d31227d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/onnxruntime/modeling_ort.py:622\u001b[0m, in \u001b[0;36mORTModel.from_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, cache_dir, subfolder, config, local_files_only, provider, session_options, provider_options, use_io_binding, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    574\u001b[0m \u001b[39m@add_start_docstrings\u001b[39m(FROM_PRETRAINED_START_DOCSTRING)\n\u001b[1;32m    575\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    590\u001b[0m ):\n\u001b[1;32m    591\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    592\u001b[0m \u001b[39m    provider (`str`, defaults to `\"CPUExecutionProvider\"`):\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[39m        ONNX Runtime provider to use for loading the model. See https://onnxruntime.ai/docs/execution-providers/ for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[39m        `ORTModel`: The loaded ORTModel model.\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 622\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    623\u001b[0m         model_id,\n\u001b[1;32m    624\u001b[0m         export\u001b[39m=\u001b[39;49mexport,\n\u001b[1;32m    625\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    626\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    627\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    628\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    629\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    630\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    631\u001b[0m         provider\u001b[39m=\u001b[39;49mprovider,\n\u001b[1;32m    632\u001b[0m         session_options\u001b[39m=\u001b[39;49msession_options,\n\u001b[1;32m    633\u001b[0m         provider_options\u001b[39m=\u001b[39;49mprovider_options,\n\u001b[1;32m    634\u001b[0m         use_io_binding\u001b[39m=\u001b[39;49muse_io_binding,\n\u001b[1;32m    635\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    636\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/modeling_base.py:372\u001b[0m, in \u001b[0;36mOptimizedModel.from_pretrained\u001b[0;34m(cls, model_id, export, force_download, use_auth_token, cache_dir, subfolder, config, local_files_only, trust_remote_code, revision, **kwargs)\u001b[0m\n\u001b[1;32m    369\u001b[0m     trust_remote_code \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    371\u001b[0m from_pretrained_method \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_transformers \u001b[39mif\u001b[39;00m export \u001b[39melse\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_from_pretrained\n\u001b[0;32m--> 372\u001b[0m \u001b[39mreturn\u001b[39;00m from_pretrained_method(\n\u001b[1;32m    373\u001b[0m     model_id\u001b[39m=\u001b[39;49mmodel_id,\n\u001b[1;32m    374\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    375\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    376\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    377\u001b[0m     force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    378\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    379\u001b[0m     subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    380\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    381\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    382\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/optimum/onnxruntime/modeling_ort.py:474\u001b[0m, in \u001b[0;36mORTModel._from_pretrained\u001b[0;34m(cls, model_id, config, use_auth_token, revision, force_download, cache_dir, file_name, subfolder, local_files_only, provider, session_options, provider_options, use_io_binding, model_save_dir, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m     onnx_files \u001b[39m=\u001b[39m [p \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m repo_files \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mmatch(pattern)]\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(onnx_files) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 474\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not find any ONNX model file in \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    475\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(onnx_files) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    476\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    477\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mToo many ONNX model files were found in \u001b[39m\u001b[39m{\u001b[39;00mmodel_path\u001b[39m}\u001b[39;00m\u001b[39m, specify which one to load by using the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfile_name argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Could not find any ONNX model file in BAAI/bge-small-en-v1.5"
     ]
    }
   ],
   "source": [
    "class EmbeddingModel:\n",
    "    \"\"\"Text embedding model using ONNX & BetterTransformers.\"\"\"\n",
    "\n",
    "    def _cls_pooling(self, model_output: Any) -> Any:\n",
    "        \"\"\"Use CLS token as pooling token.\"\"\"\n",
    "        return model_output[0][:, 0]\n",
    "\n",
    "    def _mean_pooling(self, model_output: Any, attention_mask: Any) -> Any:\n",
    "        \"\"\"Mean pooling - take attention mask into account for correct averaging.\"\"\"\n",
    "\n",
    "        token_embeddings = model_output[0]\n",
    "\n",
    "\n",
    "    def __init__(self, batch_size: int = 8, pooling: str = \"cls\"):\n",
    "        assert pooling in [\"cls\", \"mean\"], f\"Pooling {pooling} not supported.\"\n",
    "        self.pooling = pooling\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(settings.EMBEDDING_MODEL)\n",
    "        self.model = ORTModelForFeatureExtraction.from_pretrained(\n",
    "            settings.EMBEDDING_MODEL, provider=\"CUDAExecutionProvider\"\n",
    "        )\n",
    "\n",
    "    def __call__(self, texts: List[str], pooling: str = \"cls\") -> torch.Tensor:\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def pool(self, pooling: str = None) -> Callable:\n",
    "        \"\"\"Return pooling function.\"\"\"\n",
    "\n",
    "        if self.pooling == \"cls\":\n",
    "            return self._cls_pooling\n",
    "        elif self.pooling == \"mean\":\n",
    "            return self._mean_pooling\n",
    "\n",
    "embedding_model = EmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[-3.3973e-02, -5.6586e-04,  3.1019e-02, -2.4794e-02, -1.4177e-02,\n",
      "         -2.3554e-02,  3.8484e-02,  3.4079e-02,  5.4125e-02, -2.4609e-02,\n",
      "         -1.7066e-02, -9.3637e-04, -1.2509e-02, -3.1339e-03,  4.5843e-02,\n",
      "          2.2921e-02, -2.8289e-03, -6.9041e-03, -6.2671e-02,  1.0874e-02,\n",
      "          8.1873e-02,  4.2244e-04, -1.5589e-02,  2.4227e-02, -2.8118e-02,\n",
      "          5.4896e-03,  8.2625e-03, -1.1408e-02,  3.1810e-02, -5.2701e-02,\n",
      "         -2.9779e-02, -1.1553e-02, -6.4227e-03,  1.5681e-02,  2.8830e-02,\n",
      "          3.2654e-02, -6.8263e-02,  1.7395e-02, -8.2453e-02,  1.7053e-02,\n",
      "          4.5368e-02,  2.6865e-02, -3.6795e-03,  2.8645e-02, -1.2727e-02,\n",
      "         -2.6733e-02, -3.5081e-02, -3.4105e-02,  1.3175e-02, -3.1362e-02,\n",
      "         -1.9585e-02, -1.2272e-02, -3.9532e-03, -6.9001e-02,  1.6568e-03,\n",
      "          5.0986e-02,  4.8454e-02,  2.8381e-02,  1.8609e-02,  1.1230e-02,\n",
      "          5.2595e-02, -1.1500e-02, -1.6438e-01,  5.2424e-03,  2.2789e-02,\n",
      "          4.9615e-02, -2.5269e-02,  1.6288e-02,  7.7481e-03,  4.1754e-02,\n",
      "          1.2054e-02, -2.0798e-02,  4.5104e-02,  9.9546e-02, -9.2740e-02,\n",
      "          1.1025e-02,  1.1388e-02,  3.0597e-03, -2.9885e-02, -7.1217e-03,\n",
      "          7.4118e-03,  1.1355e-02,  1.5866e-02,  1.7620e-02, -5.8873e-02,\n",
      "         -3.0808e-02, -1.2298e-02,  2.3251e-02,  6.3357e-02, -5.5984e-03,\n",
      "         -2.5453e-02, -5.1804e-02, -1.2430e-02,  1.5668e-02, -3.6347e-02,\n",
      "         -4.4999e-02,  3.1863e-02, -2.3291e-02,  3.2232e-02,  3.3636e-01,\n",
      "         -1.1513e-02,  3.4026e-02,  2.8381e-02, -9.7646e-02,  1.4639e-02,\n",
      "         -1.2239e-02,  4.4840e-02, -4.6397e-02, -2.2222e-02, -2.0218e-02,\n",
      "         -6.8263e-02,  1.4481e-02,  3.2153e-02, -7.6360e-03,  8.5460e-03,\n",
      "         -3.8932e-02,  4.6845e-02,  5.5325e-03, -2.9832e-02, -1.6274e-02,\n",
      "          2.5374e-02, -9.7066e-03, -3.6927e-02, -1.5760e-02,  1.8042e-02,\n",
      "         -3.0702e-02,  6.9318e-02,  5.6604e-02,  5.1224e-02, -5.6380e-03,\n",
      "          5.3334e-02, -3.5609e-02, -6.6140e-03,  4.9773e-02, -4.9166e-02,\n",
      "         -2.6680e-02,  1.4797e-02,  3.8246e-02,  1.8239e-02, -2.9595e-02,\n",
      "         -1.5087e-02, -1.0762e-01, -6.4570e-02, -5.3307e-02, -5.0828e-02,\n",
      "          9.9968e-02, -2.4082e-02,  1.8332e-02, -1.5430e-02, -2.2486e-02,\n",
      "          3.0808e-02,  3.8985e-02,  2.9964e-02, -4.1200e-02,  1.9400e-02,\n",
      "          5.0617e-02,  3.4422e-02, -8.8823e-03,  1.3452e-02,  5.1646e-02,\n",
      "          3.5661e-02, -2.4966e-02, -2.6588e-02,  8.1715e-02,  1.5852e-02,\n",
      "         -1.0962e-01, -3.7620e-03, -2.7854e-02,  5.6130e-02, -6.9832e-03,\n",
      "          5.9295e-02,  3.9117e-02, -2.6390e-02,  2.4715e-02,  1.6591e-02,\n",
      "          6.6364e-02, -1.4771e-02, -3.1092e-03, -5.8128e-03,  3.1969e-02,\n",
      "          3.8826e-02, -3.6532e-02, -7.9869e-02,  6.9476e-02,  7.9658e-03,\n",
      "         -2.0943e-02, -5.9471e-04, -3.6189e-02,  4.2836e-02, -1.4507e-02,\n",
      "         -3.6875e-02,  1.0176e-01,  3.1520e-03,  5.1764e-03, -8.4564e-02,\n",
      "          4.4511e-03,  3.6294e-02, -7.1428e-02, -2.1405e-02, -2.0745e-02,\n",
      "         -2.2842e-02,  2.0310e-02, -1.5958e-02, -6.1458e-03,  2.4346e-02,\n",
      "         -8.3548e-03,  1.3874e-02, -1.3966e-02,  4.1121e-02, -1.3755e-02,\n",
      "         -5.0459e-02, -3.5556e-02, -1.7771e-03,  5.5444e-02,  2.2140e-03,\n",
      "         -8.0660e-02, -3.9934e-02,  3.8352e-02,  2.5572e-02,  1.5628e-02,\n",
      "         -1.0867e-02, -2.1589e-02,  4.5975e-02, -3.5535e-01, -3.1230e-02,\n",
      "          2.4201e-02, -4.3759e-02,  4.3970e-02, -7.6822e-03,  1.5852e-02,\n",
      "         -2.2895e-02, -1.8226e-02,  3.4751e-03,  1.7145e-02, -1.0683e-01,\n",
      "          7.4540e-02, -2.1774e-02,  3.3287e-02,  7.1019e-03, -2.5493e-02,\n",
      "         -1.4639e-03,  9.0314e-02,  3.6822e-02,  5.3967e-02, -3.1362e-02,\n",
      "          1.3518e-02, -5.7712e-02,  4.5843e-02, -6.4788e-04,  1.9297e-01,\n",
      "          9.8068e-02, -2.0033e-02, -3.5002e-02,  3.1256e-02, -3.2760e-02,\n",
      "          1.2331e-02, -9.4481e-02,  8.2955e-03,  1.9980e-02, -2.1444e-02,\n",
      "          3.2610e-05, -2.0904e-02,  1.0656e-02, -7.6295e-03,  6.2565e-02,\n",
      "         -2.1233e-02,  1.5114e-02, -1.8978e-02, -4.8955e-02, -4.9430e-02,\n",
      "         -1.6116e-02, -6.0825e-02,  6.2671e-02,  7.1217e-03, -2.9542e-02,\n",
      "         -1.4824e-02,  1.4995e-02,  4.8586e-02, -3.2048e-02, -5.7923e-02,\n",
      "         -5.2173e-02, -3.2839e-02, -7.3657e-03, -6.0370e-03,  6.3515e-02,\n",
      "          3.8879e-02,  4.3357e-03,  4.8612e-02,  4.0831e-02,  4.8981e-02,\n",
      "         -8.7940e-02, -4.5843e-02,  5.9822e-02, -4.1042e-02,  6.3357e-02,\n",
      "         -6.5150e-03, -8.4880e-02,  3.6598e-03, -7.0162e-03, -1.7329e-02,\n",
      "         -2.6654e-02, -2.9252e-02, -2.7880e-02,  3.5397e-02,  3.3235e-02,\n",
      "         -1.9057e-02,  2.0785e-02,  6.2354e-02,  5.3782e-02,  2.3488e-02,\n",
      "         -8.5131e-03,  6.6680e-02, -2.1180e-02, -2.2025e-02, -1.1804e-02,\n",
      "         -1.3571e-02,  2.5008e-03, -2.3805e-03, -4.5684e-02, -3.0386e-01,\n",
      "          3.4817e-02, -5.7633e-03,  3.1230e-02,  1.3116e-02,  5.8925e-02,\n",
      "         -5.1039e-02,  1.3808e-02, -4.9351e-02,  1.2819e-02, -4.1121e-02,\n",
      "          3.4791e-02,  3.0122e-02, -1.0597e-02, -3.8668e-02,  2.9647e-02,\n",
      "          4.1807e-02, -1.7712e-02,  3.9169e-02, -8.9892e-02,  3.0623e-02,\n",
      "          6.4920e-03,  1.2756e-01, -5.2674e-02,  9.4956e-02,  1.5509e-02,\n",
      "         -3.9354e-02, -2.8434e-02,  4.7346e-02,  3.0966e-02,  4.0066e-02,\n",
      "         -1.7132e-02,  8.3772e-02,  4.3152e-02,  1.3597e-02, -5.2041e-02,\n",
      "          2.4952e-02,  1.8305e-02,  2.1365e-02,  2.3871e-02, -6.5836e-02,\n",
      "          3.0650e-02, -8.1768e-02,  1.0214e-02,  9.1738e-02,  2.4121e-02,\n",
      "          6.2038e-02, -8.1082e-02, -5.8556e-02, -9.6209e-03,  1.3478e-02,\n",
      "         -1.7422e-02,  1.6248e-02,  4.9094e-03,  4.3495e-02, -4.3620e-03,\n",
      "         -2.7142e-02, -4.6476e-02, -3.4685e-03, -3.3314e-02, -2.2051e-02,\n",
      "         -3.6532e-02,  1.8266e-02,  3.4870e-02,  1.3406e-02],\n",
      "        [-1.5780e-02,  1.6596e-02, -1.4069e-02,  1.7965e-02, -1.7149e-02,\n",
      "         -2.6415e-02,  3.7009e-02,  2.8934e-04,  6.4174e-02,  8.4495e-03,\n",
      "         -4.4274e-02,  1.3010e-02,  7.2176e-02, -4.2827e-02,  6.4648e-02,\n",
      "          3.5825e-02, -3.3133e-03, -1.9742e-02, -5.6330e-02, -1.2082e-02,\n",
      "          3.0323e-02, -1.4161e-02,  2.8534e-02,  5.6495e-03, -1.7320e-02,\n",
      "          4.4564e-02, -7.1268e-03, -3.2219e-02, -3.7799e-02, -1.6794e-01,\n",
      "          4.9026e-03, -3.9220e-02,  1.8873e-02,  3.1034e-02,  2.7902e-02,\n",
      "          2.2940e-02, -8.8075e-02,  4.6407e-02, -4.4538e-02,  2.7270e-02,\n",
      "          3.9589e-02,  4.3116e-02, -7.2453e-03, -4.1458e-02,  1.5728e-02,\n",
      "         -4.0247e-02, -6.2753e-02,  4.2274e-02,  6.7386e-02,  5.9193e-03,\n",
      "         -7.0965e-02, -3.3982e-02, -6.5601e-04,  1.1325e-02,  3.2482e-02,\n",
      "          2.8902e-02, -2.5862e-03,  1.0113e-01,  4.8512e-02, -3.4647e-03,\n",
      "          2.3322e-02,  7.4756e-02, -1.8068e-01,  5.0592e-02, -2.1440e-02,\n",
      "         -2.1611e-02,  8.6864e-04,  1.8327e-03, -3.2535e-02,  5.8857e-02,\n",
      "         -7.0018e-03, -5.3435e-02,  4.1668e-02,  4.6775e-02,  2.3937e-03,\n",
      "          5.5812e-04, -3.3403e-02, -2.6007e-02, -1.3806e-02,  3.0876e-02,\n",
      "          4.3248e-02, -4.7380e-02, -1.2457e-02,  1.5061e-03, -1.8676e-02,\n",
      "         -3.5799e-02, -2.1479e-02,  1.5938e-02,  1.1819e-02,  2.2729e-02,\n",
      "         -5.1724e-02, -4.6907e-02, -6.5740e-03,  6.1298e-03, -5.3673e-04,\n",
      "         -5.3145e-02, -2.3901e-02,  2.9771e-02,  7.2597e-02,  3.5756e-01,\n",
      "         -1.7570e-02,  2.4006e-02,  7.5809e-02, -6.8754e-02, -1.1463e-02,\n",
      "         -6.1068e-03,  3.7115e-02, -1.8492e-02, -3.1587e-02,  5.5014e-02,\n",
      "         -6.8333e-02, -4.4880e-02,  3.3008e-02, -7.5085e-03,  6.4161e-03,\n",
      "          3.2689e-03,  3.9010e-02,  8.5022e-03, -8.5351e-03,  8.8641e-03,\n",
      "          6.6530e-03,  4.5696e-02, -7.5282e-02, -3.8760e-03,  3.4061e-02,\n",
      "         -4.5433e-02,  4.9697e-02,  8.1494e-02,  6.4332e-02,  1.1138e-03,\n",
      "          4.0747e-02,  6.3069e-02, -4.6670e-02,  1.6070e-02,  1.0950e-02,\n",
      "         -3.5430e-02,  3.9958e-02, -3.1508e-02,  5.9436e-02, -2.4835e-02,\n",
      "          1.7123e-02, -2.4598e-02, -2.1466e-02,  1.0052e-03, -3.7641e-02,\n",
      "          1.4509e-01, -5.7594e-02, -3.0429e-02, -1.1562e-02, -1.0588e-02,\n",
      "         -2.0874e-02,  6.3595e-02,  4.4617e-02, -9.7709e-02, -2.1242e-02,\n",
      "          1.9321e-02,  3.2271e-02, -4.1484e-02, -4.1458e-02, -1.7866e-03,\n",
      "          5.2276e-02,  3.7852e-02, -2.0795e-02,  8.3074e-02,  4.6538e-02,\n",
      "         -2.4243e-02, -1.1023e-02, -3.8233e-03, -9.5090e-03, -6.2332e-02,\n",
      "          5.7284e-03,  2.7515e-04, -7.8757e-02,  2.8349e-02,  2.9560e-02,\n",
      "         -3.9056e-03, -2.1900e-02, -5.0513e-02,  3.4298e-02,  5.7679e-03,\n",
      "          6.9807e-02, -2.1505e-02, -6.8281e-02,  5.1487e-02, -5.7120e-03,\n",
      "         -1.3345e-02, -5.3882e-02, -2.5941e-02,  1.3675e-02,  9.7327e-03,\n",
      "         -8.8970e-03,  5.9647e-02, -2.5638e-02, -3.6983e-02, -5.4066e-02,\n",
      "         -4.9723e-02,  2.1598e-02,  6.0904e-03, -4.5459e-02,  7.0347e-03,\n",
      "          1.1753e-02, -1.2582e-02, -7.4493e-02, -4.7144e-02,  6.3385e-02,\n",
      "         -5.6593e-02, -1.6794e-02,  8.0218e-03,  5.7594e-02,  8.1863e-02,\n",
      "         -1.2536e-02, -2.0466e-02,  3.9484e-02,  2.4217e-02, -2.5730e-02,\n",
      "         -5.7067e-02, -2.9350e-02,  2.8455e-02,  4.3774e-02,  1.1240e-02,\n",
      "         -2.6507e-02, -2.5243e-02, -8.6535e-03, -3.1145e-01, -7.7980e-03,\n",
      "          7.7072e-02, -2.3243e-02,  6.1173e-02, -9.0154e-03, -4.1326e-03,\n",
      "         -7.4164e-03,  5.8067e-02,  1.8702e-02, -1.4428e-03, -5.4751e-02,\n",
      "          1.3174e-02, -2.9034e-02,  3.0139e-03,  1.0707e-02, -2.8534e-02,\n",
      "          8.1494e-02,  4.8499e-03,  1.4648e-02,  3.6141e-02, -6.7715e-03,\n",
      "          4.0247e-02, -9.9604e-02, -3.3061e-02,  3.8826e-03,  1.7268e-01,\n",
      "          8.3284e-02,  3.9194e-02,  1.9650e-02,  4.0747e-02, -3.1271e-02,\n",
      "         -1.0226e-02, -9.6814e-02,  8.0231e-02,  3.3035e-02,  4.0931e-02,\n",
      "          1.2273e-03, -2.3940e-02,  2.9432e-03,  5.4014e-02,  4.5959e-02,\n",
      "          9.2869e-04, -4.1010e-02, -6.7017e-02, -6.4918e-03, -3.9168e-02,\n",
      "         -5.7436e-02, -2.7586e-02,  1.4122e-02,  2.0321e-02, -4.2590e-02,\n",
      "          3.7036e-02, -2.8346e-03,  1.2299e-02, -5.1329e-02, -5.5593e-02,\n",
      "         -2.2242e-02, -2.2703e-02, -3.0982e-02, -7.0544e-03,  6.5701e-02,\n",
      "         -2.2558e-02, -1.0970e-02,  4.7380e-03,  4.9434e-02,  6.2648e-02,\n",
      "         -2.2756e-02,  7.1597e-03,  7.7915e-03, -2.9876e-02,  1.8900e-02,\n",
      "         -3.1113e-02, -2.0189e-02,  1.0833e-03,  3.1403e-02, -8.5443e-02,\n",
      "          2.1677e-02, -5.5383e-02, -2.1584e-02,  5.8173e-02,  6.0542e-03,\n",
      "          7.2584e-03,  1.2470e-03,  3.1482e-02,  3.5351e-02,  3.1166e-02,\n",
      "         -1.4122e-02, -1.3161e-03,  1.8584e-02,  4.1853e-02,  2.9613e-02,\n",
      "         -2.0308e-02, -8.4561e-03,  1.9123e-02, -1.6952e-02, -2.9650e-01,\n",
      "         -3.3535e-02, -2.9323e-02,  7.7520e-03,  4.1405e-02,  4.6170e-02,\n",
      "         -1.2141e-02,  4.5775e-02, -6.5490e-02,  3.4904e-02, -4.1879e-02,\n",
      "          5.9805e-02, -1.4398e-02, -2.6717e-02,  1.6534e-03,  2.0742e-02,\n",
      "          9.9236e-02,  1.0733e-02,  3.5772e-02, -3.5772e-02,  9.6867e-03,\n",
      "          1.4227e-02,  1.6657e-01,  1.0220e-02,  2.2085e-02, -2.7928e-02,\n",
      "          1.4188e-02, -1.9400e-02,  2.8086e-02, -4.0175e-03, -5.8041e-03,\n",
      "         -5.8225e-02,  5.7330e-02,  2.3703e-02, -1.4385e-02,  2.6388e-03,\n",
      "         -2.5033e-02,  1.3063e-02,  1.7873e-02, -3.9484e-03, -4.6907e-02,\n",
      "          6.7780e-03, -2.3914e-02, -5.8962e-02,  8.0231e-02, -2.3269e-02,\n",
      "          2.6875e-02, -6.4648e-02, -5.6225e-02, -9.6274e-03,  3.3989e-03,\n",
      "          3.7641e-03,  1.5188e-02, -5.1829e-02,  6.5181e-03,  4.9907e-02,\n",
      "         -5.0013e-02, -1.5991e-02,  3.4167e-02,  5.8962e-03, -3.3351e-02,\n",
      "         -3.5272e-02, -6.1726e-03,  6.3069e-02, -3.3272e-02]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 13:18:32.575404481 [W:onnxruntime:, session_state.cc:1162 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-11-26 13:18:32.575419138 [W:onnxruntime:, session_state.cc:1164 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    }
   ],
   "source": [
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('models/BAAI/bge-small-en-v1.5-onnx-O4')\n",
    "ort_model = ORTModelForFeatureExtraction.from_pretrained('models/BAAI/bge-small-en-v1.5-onnx-O4', provider=\"CUDAExecutionProvider\")\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "# Compute token embeddings\n",
    "model_output = ort_model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, cls pooling.\n",
    "sentence_embeddings = model_output[0][:, 0]\n",
    "\n",
    "# normalize embeddings\n",
    "sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 13:18:41.629139453 [W:onnxruntime:, session_state.cc:1162 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\n",
      "2023-11-26 13:18:41.629151961 [W:onnxruntime:, session_state.cc:1164 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = pipeline(\n",
    "    'feature-extraction', \n",
    "    model=ort_model, \n",
    "    tokenizer=tokenizer, \n",
    "    device=0,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=True,\n",
    "    framework=\"pt\",\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# extractor(sentences) -> need CLS pooling\n",
    "extractor(sentences[0])[:, 0].numpy() == model_output[0][0, 0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = data.load.load_test_sentences()\n",
    "\n",
    "class Node(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    embedding: Optional[list] = None\n",
    "    author: str\n",
    "    year: int\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return getattr(self, key)\n",
    "\n",
    "def generator():\n",
    "    for x in data.load.load_test_sentences():\n",
    "        yield Node(**x)\n",
    "\n",
    "ds = KeyDataset(generator(), \"description\")\n",
    "\n",
    "# nodes \n",
    "\n",
    "# for d in generator():\n",
    "#     print(d)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f74222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ds:\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f74222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(x)\n\u001b[1;32m      <a href='vscode-notebook-cell://dev-container%2B7b22686f737450617468223a222f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f74222c226c6f63616c446f636b6572223a66616c73652c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2265787465726e616c223a2266696c653a2f2f2f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f686f6d652f6d7561646469622f7369657463682f4f6d696373436f70696c6f742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d/workspaces/OmicsCopilot/notebooks/onnx-embeddings.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py:305\u001b[0m, in \u001b[0;36mKeyDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[i][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "for x in ds:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: stuff to look into\n",
    "#     - implementing cls and mean pooling -> what is the difference?\n",
    "#       - which works better?\n",
    "#     - how to use the pipeline to get the embeddings\n",
    "#     - figure out best way to get embeddings for a set of Node objects, \n",
    "#       - need to be able to update the Node object with the embeddings\n",
    "#       - Node.upsert_embedding(db) -> store in db??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
